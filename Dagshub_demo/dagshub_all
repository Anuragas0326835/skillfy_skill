import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Import all the models we will use
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb
from sklearn.metrics import accuracy_score, f1_score
from sklearn.pipeline import Pipeline
import mlflow
import dagshub


# --- 1. Configuration & Constants ---
FILEPATH = r'C:\Users\dell\Desktop\skillfy links\New folder\skillfy_skill\dataset\diabetes.csv'
EXPERIMENT_NAME = "Diabetes Prediction Experiment"
RANDOM_STATE = 42

# --- 2. DagsHub & MLflow Initialization ---
print("Initializing DagsHub and MLflow...")
dagshub.init(repo_owner='Anuragas0326835', repo_name='Dagshub_demo', mlflow=True)
mlflow.set_experiment(EXPERIMENT_NAME)

# --- 3. Load and Preprocess Data ---
print(f"Loading data from {FILEPATH}")
df = pd.read_csv(FILEPATH)
df = df.dropna()
df = df.drop_duplicates()
print("Data loaded and cleaned.")

# --- 4. Data Splitting ---
X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)

# --- 5. Define Models ---
# We will train and log each of these models in a loop.
models = {
    "DecisionTree": DecisionTreeClassifier(random_state=RANDOM_STATE),
    "RandomForest": RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),
    "SVM": SVC(random_state=RANDOM_STATE),
    "XGBoost": xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')
}

# --- 6. Model Training, Evaluation, and Logging Loop ---
for model_name, model in models.items():
    print(f"\n{'='*20}")
    print(f"--- Training and evaluating: {model_name} ---")
    print(f"{'='*20}")

    # Create the pipeline with a scaler and the current model
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', model)
    ])

    # Train the pipeline
    pipeline.fit(X_train, y_train)
    print("Training complete.")

    # Evaluate the model
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print("\n--- Evaluation Metrics ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Start a new MLflow run for each model
    with mlflow.start_run(run_name=f"{model_name} Run") as run:
        print("\n--- Starting MLflow Run ---")
        run_id = run.info.run_id
        print(f"Run ID: {run_id}")

        # Log parameters - logging all model params is a best practice
        mlflow.log_param("model_type", f"{model_name}_in_Pipeline")
        mlflow.log_params(model.get_params())

        # Log metrics
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("f1_score", f1)

        # Log the model pipeline
        mlflow.sklearn.log_model(
            sk_model=pipeline,
            artifact_path="diabetes_model"  # This path matches what predict.py expects
        )

        print(f"Model and metrics logged to MLflow for run: {run_id}")
        print("--- MLflow Run Complete ---")

print("\nAll models have been trained and logged.")
print("View your experiment at:", mlflow.get_tracking_uri())